# RAG Improvements - Quick Start Guide

**‚è±Ô∏è 15-Minute Setup** | **üí∞ 30-50% Cost Reduction** | **‚úÖ Production Ready**

---

## What You're Getting

6 major improvements to your RAG system:

1. **Answer Verification** - Prevents hallucinations
2. **Adaptive Retrieval** - Optimizes costs automatically
3. **Query Cache** - 50% cost reduction for repeated queries
4. **Better Conversations** - Handles long sessions
5. **Multi-Hop Reasoning** - Answers complex questions
6. **Quality Monitoring** - Track what's working

---

## Quick Start (15 minutes)

### Step 1: Test Everything Works (2 min)

```bash
# Make sure you're in the project directory
cd C:/Users/tech/Projects/docling-n8n

# Run the test suite
python test_improvements_complete.py
```

**Expected output**:
```
‚úÖ Answer Verification Test PASSED
‚úÖ Adaptive Retrieval Test PASSED
‚úÖ Query Cache Test PASSED
‚úÖ Enhanced Conversation Memory Test PASSED
‚úÖ Multi-Hop Reasoning Test PASSED
‚úÖ Retrieval Metrics Test PASSED

‚úÖ ALL TESTS PASSED!
```

---

### Step 2: Run Database Migrations (1 min)

```bash
python run_new_migrations.py
```

**Expected output**:
```
‚úÖ Migration 010_add_query_cache.sql completed successfully
‚úÖ Migration 011_add_retrieval_metrics.sql completed successfully

‚úÖ All migrations completed successfully!
```

**Verify**:
```bash
# Check tables exist
psql $DATABASE_URL -c "\dt query_cache"
psql $DATABASE_URL -c "\dt retrieval_metrics"
```

---

### Step 3: Integrate Core Features (10 min)

Open `backend/app/api/chat_multimodal.py` and make these changes:

#### 3a. Add Imports (Line ~14)

```python
# NEW IMPORTS - Add these
from app.utils.answer_verifier import quick_verify
from app.utils.adaptive_retrieval import adaptive_retrieval_params
from app.utils.query_cache import QueryCache
from app.utils.retrieval_metrics import RetrievalMetrics
```

#### 3b. Initialize Components (Line ~28)

```python
# NEW - Initialize cache and metrics
query_cache = QueryCache(db_client=None, ttl_hours=24)
retrieval_metrics = RetrievalMetrics(db_client=None)
```

#### 3c. Add Cache Check (After line 410)

Find this line:
```python
conversation_context = extract_conversation_context(chat_history_dicts)
```

Add AFTER it:
```python
# NEW: Check cache
query_cache.db = db
cached = query_cache.get_cached_answer(
    question=request.question,
    doc_id=request.doc_id,
    question_embedding=question_embedding
)

if cached:
    print("‚úÖ CACHE HIT!")
    return ChatResponse(
        answer=cached.answer,
        citations=cached.citations,
        images_used=[],
        tables_used=0,
        tokens_used=0,
        search_results_count=len(cached.citations),
        model_used=cached.model_used
    )
```

#### 3d. Use Adaptive Retrieval (Replace lines ~437-442)

Replace this:
```python
# Existing code
top_k = request.top_k
context_window = request.context_window

if conversation_context['is_followup']:
    top_k = min(top_k + 2, 10)
    context_window = min(context_window + 1, 4)
```

With this:
```python
# NEW: Adaptive retrieval
top_k, context_window, complexity = adaptive_retrieval_params(
    question=request.question,
    query_type=query_type,
    is_followup=conversation_context['is_followup']
)
print(f"üéØ Adaptive: complexity={complexity}, top_k={top_k}, window={context_window}")
```

#### 3e. Add Answer Verification (After line ~610)

Find this line:
```python
cleaned_answer = clean_markdown(response.choices[0].message.content)
```

Add AFTER it:
```python
# NEW: Verify answer
print("üîç Verifying answer...")
is_grounded, confidence, disclaimer = quick_verify(
    answer=cleaned_answer,
    citations=[{'content': ctx['chunk']['content']} for ctx in expanded_contexts]
)
print(f"   Confidence: {confidence:.1%}")

if disclaimer:
    cleaned_answer += disclaimer
```

#### 3f. Cache the Answer (Before return statement)

Find the `return ChatResponse(...)` at the end.

Add BEFORE it:
```python
# NEW: Cache this answer
query_cache.cache_answer(
    question=request.question,
    doc_id=request.doc_id,
    answer=cleaned_answer,
    citations=[{'content': c.content, 'page_number': c.page_number} for c in citations],
    question_embedding=question_embedding
)
```

---

### Step 4: Verify Integration (2 min)

**Test the API**:
```bash
# Start the backend
cd backend
uvicorn app.main:app --reload
```

**Send test query** (in another terminal):
```bash
curl -X POST http://localhost:8000/api/chat/multimodal \
  -H "Content-Type: application/json" \
  -d '{
    "doc_id": "YOUR_DOC_ID",
    "question": "What is System Database?",
    "top_k": 5
  }'
```

**Look for in logs**:
```
üéØ Adaptive: complexity=simple, top_k=3, window=1
üîç Verifying answer...
   Confidence: 95%
```

**Send same query again** - should see:
```
‚úÖ CACHE HIT!
```

---

## What to Expect

### First Query (Cache Miss)
```
Request ‚Üí Adaptive Retrieval (optimized) ‚Üí LLM ‚Üí Verify ‚Üí Cache ‚Üí Response
Cost: ~$0.0012 (cheaper due to adaptive retrieval)
Time: ~2-3 seconds
```

### Second Identical Query (Cache Hit)
```
Request ‚Üí Cache Lookup ‚Üí Response
Cost: $0 (zero LLM calls!)
Time: <100ms
```

### Simple Query
```
Complexity: simple
top_k: 2-3 (instead of 5)
Cost: -40% vs before
```

### Complex Query
```
Complexity: complex
top_k: 7-8 (more context)
Multi-hop: May trigger automatic decomposition
Cost: +20% but much better quality
```

---

## Monitor Performance

### Check Cache Hit Rate
```sql
SELECT
    COUNT(*) as total_cached,
    SUM(hit_count) as total_hits,
    (SUM(hit_count)::float / NULLIF(COUNT(*), 0)) as hit_rate
FROM query_cache;
```

**Good**: Hit rate > 20% after 1 week
**Great**: Hit rate > 40% after 1 week

### Check Retrieval Quality
```sql
SELECT * FROM daily_retrieval_summary LIMIT 7;
```

**Look for**:
- `avg_coverage` > 0.75 (75%+ topic match)
- `avg_score` > 0.70
- `avg_diversity` > 2 (multiple topics per query)

### Identify Issues
```sql
SELECT * FROM low_coverage_queries LIMIT 10;
```

**Action**: If queries appear here frequently, may need to tune topic mapping

---

## Expected Savings

**Month 1** (assuming 10,000 queries):
- Baseline cost: ~$15
- With improvements: ~$8-10
- **Savings: $5-7/month** (33-47% reduction)

**At Scale** (100,000 queries/month):
- Baseline cost: ~$150
- With improvements: ~$80-100
- **Savings: $50-70/month**

---

## Troubleshooting

### Tests Fail
```bash
# Check Python dependencies
pip install -r requirements.txt

# Check OpenAI API key
echo $OPENAI_API_KEY
```

### Migrations Fail
```bash
# Check database connection
psql $DATABASE_URL -c "SELECT 1;"

# Check if tables already exist
psql $DATABASE_URL -c "\dt query_cache"
```

### Cache Not Working
```python
# Check in code - make sure db is set
query_cache.db = db  # This line must be present
```

### Answer Verification Too Strict
```python
# Adjust confidence threshold
if confidence < 0.8:  # Lower from 0.9 to 0.8
    # Add disclaimer
```

---

## Next Steps

1. ‚úÖ **Run this quick start** (you're here!)
2. **Monitor for 1 week** - Check cache hit rate, cost reduction
3. **Read full docs** - `INTEGRATION_GUIDE.md` for advanced features
4. **Add Phase 2** - Enhanced conversation memory (optional)
5. **Add Phase 3** - Multi-hop reasoning (optional)

---

## Support

**Full Documentation**: See `INTEGRATION_GUIDE.md`
**Implementation Details**: See `RAG_IMPROVEMENTS_SUMMARY.md`
**Code**: All modules in `backend/app/utils/`

---

**You're Done!** üéâ

Your system now has:
- ‚úÖ Answer verification
- ‚úÖ Adaptive retrieval
- ‚úÖ Query caching
- ‚úÖ Quality monitoring

**Expected improvement**: 30-50% cost reduction + better accuracy

Monitor the dashboards and tune as needed. The system will automatically optimize based on query patterns.
